{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Pneumonia Detection: Anchor-Free vs. Anchor-Based Object Detection\n\n**NAML Course Project — Politecnico di Milano**\n\nThis notebook runs the full training and evaluation pipeline on **Kaggle** with GPU acceleration.\n\nThree models compared:\n1. **FCOS** — anchor-free (paper's method)\n2. **RetinaNet** — anchor-based, one-stage\n3. **Faster R-CNN** — anchor-based, two-stage\n\n### Advanced Training Techniques\n- **COCO-pretrained models**: Full detection model weights (backbone + FPN + heads) for fast convergence\n- **Backbone freezing**: Early ResNet layers frozen for first 3 epochs (prevents overfitting)\n- **EMA (Exponential Moving Average)**: Smoother optimization for better generalization\n- **Cosine annealing LR**: Better convergence than step decay\n- **WeightedRandomSampler**: Oversamples positive patients (3x) for class imbalance\n- **Medical augmentations**: CLAHE, rotation, contrast, noise, elastic/grid distortion (via albumentations)\n- **TTA (Test-Time Augmentation)**: Horizontal flip at eval time (+1-3% AP)\n- **Gaussian Soft-NMS**: Decays overlapping scores instead of hard removal\n- **Optimal patient threshold**: ROC/Youden's J statistic for patient classification\n- **Channels-last memory**: 10-30% faster GPU convolutions\n- **AMP + multi-GPU + torch.compile()**: Maximum hardware utilization\n\n### Kaggle Setup\n1. **Add the RSNA dataset**: Click *Add Data* → search `rsna-pneumonia-detection-challenge` (Competition tab) → Add\n2. **Upload project code**: Upload your `Project_Pneumonia_Detection.zip` as a Kaggle Dataset, then add it here via *Add Data*\n3. **Enable GPU**: Settings → Accelerator → **GPU T4 x2** (trains 2 models in parallel) or **GPU P100** (single GPU)\n4. **Enable Internet**: Settings → Internet → **On** (needed for pip install)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Verify GPU is available\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available:  {torch.cuda.is_available()}\")\nNUM_GPUS = torch.cuda.device_count()\nprint(f\"Number of GPUs:  {NUM_GPUS}\")\nfor i in range(NUM_GPUS):\n    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} \"\n          f\"({torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB)\")\nif NUM_GPUS == 0:\n    print(\"WARNING: No GPU detected! Go to Settings > Accelerator > GPU T4 x2\")\nelif NUM_GPUS >= 2:\n    print(\"\\n2 GPUs detected — will train models in parallel!\")\nelse:\n    print(\"\\n1 GPU detected — will train models sequentially.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RSNA Pneumonia Detection Challenge"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Copy project source code from uploaded dataset to working directory\nimport os\nimport shutil\nfrom pathlib import Path\n\nWORKING_DIR = \"/kaggle/working\"\nos.chdir(WORKING_DIR)\n\n# Search for main.py under all known Kaggle input mount points\n# Skip checkpoint datasets (they may contain stale code from a previous run)\ninput_dirs = [Path(\"/kaggle/input/datasets\"), Path(\"/kaggle/input\"), Path(\"/kaggle/input/competitions\")]\nproject_src = None\n\nfor base in input_dirs:\n    if not base.exists():\n        continue\n    for p in base.rglob(\"main.py\"):\n        # Verify it's our project (has src/ alongside it)\n        if (p.parent / \"src\").is_dir():\n            # Skip if this looks like a checkpoint dataset (has checkpoints/ folder)\n            if (p.parent / \"checkpoints\").is_dir():\n                print(f\"  Skipping checkpoint dataset: {p.parent}\")\n                continue\n            project_src = p.parent\n            break\n    if project_src:\n        break\n\n# Fallback: if no non-checkpoint source found, use any match\nif project_src is None:\n    for base in input_dirs:\n        if not base.exists():\n            continue\n        for p in base.rglob(\"main.py\"):\n            if (p.parent / \"src\").is_dir():\n                project_src = p.parent\n                break\n        if project_src:\n            break\n\nif project_src is None:\n    print(\"ERROR: Could not find project files (main.py + src/) in /kaggle/input/\")\n    print(\"Make sure you uploaded the project ZIP as a Kaggle Dataset and added it to this notebook.\")\nelse:\n    print(f\"Found project at: {project_src}\")\n    for item in [\"main.py\", \"src\", \"requirements.txt\", \"regenerate_plots.py\"]:\n        src = project_src / item\n        dst = Path(WORKING_DIR) / item\n        if src.exists():\n            # Always overwrite to ensure latest code\n            if dst.exists():\n                if dst.is_dir():\n                    shutil.rmtree(str(dst))\n                else:\n                    dst.unlink()\n            if src.is_dir():\n                shutil.copytree(str(src), str(dst))\n            else:\n                shutil.copy2(str(src), str(dst))\n            print(f\"  Copied: {item}\")\n    print(f\"Working directory: {os.getcwd()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T01:22:32.314532Z",
     "start_time": "2026-02-26T01:22:32.289868Z"
    }
   },
   "source": "# Install dependencies (albumentations and scikit-learn are usually pre-installed on Kaggle)\n!pip install -q pydicom seaborn albumentations scikit-learn",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify project structure\n",
    "required = [\"main.py\", \"src/config.py\", \"src/engine.py\", \"src/evaluate.py\",\n",
    "            \"src/dataset.py\", \"src/transforms.py\", \"src/visualize.py\",\n",
    "            \"src/models/__init__.py\", \"src/models/fcos.py\",\n",
    "            \"src/models/retinanet.py\", \"src/models/faster_rcnn.py\"]\n",
    "missing = [f for f in required if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print(f\"ERROR: Missing files: {missing}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Contents: {os.listdir('.')}\")\n",
    "else:\n",
    "    print(\"All project files found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset\n",
    "\n",
    "The RSNA dataset is already available at `/kaggle/input/rsna-pneumonia-detection-challenge/` — no download needed!\n",
    "\n",
    "Since Kaggle input is read-only, we symlink the dataset files to a writable `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Set up data directory with symlinks to the Kaggle input (read-only)\nimport os, glob\nfrom pathlib import Path\n\n# Find RSNA dataset under any Kaggle mount point\nRSNA_INPUT = None\nfor candidate in [\n    \"/kaggle/input/rsna-pneumonia-detection-challenge\",\n    \"/kaggle/input/competitions/rsna-pneumonia-detection-challenge\",\n]:\n    if os.path.exists(candidate):\n        RSNA_INPUT = candidate\n        break\n\nif RSNA_INPUT is None:\n    # Fallback: search for the labels CSV\n    for p in Path(\"/kaggle/input\").rglob(\"stage_2_train_labels.csv\"):\n        RSNA_INPUT = str(p.parent)\n        break\n\nif RSNA_INPUT is None:\n    raise FileNotFoundError(\n        \"RSNA dataset not found! Add it via: Add Data → Competition tab → rsna-pneumonia-detection-challenge\"\n    )\n\nprint(f\"RSNA dataset found at: {RSNA_INPUT}\")\n\nDATA_DIR = \"/kaggle/working/data\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Symlink the dataset files into our writable data directory\nfor item in os.listdir(RSNA_INPUT):\n    src = os.path.join(RSNA_INPUT, item)\n    dst = os.path.join(DATA_DIR, item)\n    if not os.path.exists(dst):\n        os.symlink(src, dst)\n        print(f\"  Linked: {item}\")\n\n# Auto-detect previous run output (saved as dataset) and restore PNG images\nprev_png = None\nfor candidate in Path(\"/kaggle/input\").rglob(\"stage_2_train_images_png\"):\n    if candidate.is_dir():\n        prev_png = str(candidate)\n        break\n\npng_dst = os.path.join(DATA_DIR, \"stage_2_train_images_png\")\nif prev_png and not os.path.exists(png_dst):\n    os.symlink(prev_png, png_dst)\n    n_pngs = len(os.listdir(png_dst))\n    # Verify PNG count — if incomplete, remove symlink so cell 9 reconverts\n    if n_pngs < 25000:  # full dataset has ~26,684\n        os.unlink(png_dst)\n        print(f\"  Previous PNGs incomplete ({n_pngs} files), will reconvert\")\n    else:\n        print(f\"  Linked {n_pngs} PNG images from previous run\")\n\nprint(f\"\\nData directory: {DATA_DIR}\")\nprint(f\"Contents: {os.listdir(DATA_DIR)}\")\n\n# Show dataset stats\nimport pandas as pd\ndf = pd.read_csv(f\"{DATA_DIR}/stage_2_train_labels.csv\")\nn_patients = df[\"patientId\"].nunique()\nn_positive = df[df[\"Target\"] == 1][\"patientId\"].nunique()\nprint(f\"\\nTotal patients: {n_patients}\")\nprint(f\"Positive (pneumonia): {n_positive}\")\nprint(f\"Negative: {n_patients - n_positive}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Preprocess DICOM -> PNG (much faster data loading during training)\n# This runs on Kaggle's local SSD, so it's ~10-15 min instead of ~160 min on Colab+Drive\nimport os\npng_dir = f\"{DATA_DIR}/stage_2_train_images_png\"\nif not os.path.exists(png_dir) or len(os.listdir(png_dir)) < 100:\n    !PYTHONUNBUFFERED=1 python -m src.preprocess --data-dir {DATA_DIR} --compress 1\nelse:\n    print(f\"PNG images already exist ({len(os.listdir(png_dir))} files).\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Training Configuration\n\n**Optimized for maximum performance within Kaggle's 12-hour limit.**\n\nAdvanced features enabled by default:\n- **Backbone freezing** (3 epochs): Prevents catastrophic forgetting of COCO features\n- **EMA** (decay=0.999): Exponential Moving Average for smoother generalization\n- **Cosine annealing**: Better LR schedule than step decay\n- **Weighted sampler**: 3x oversampling of pneumonia-positive patients\n- **TTA + Soft-NMS**: Test-time augmentation and Gaussian score decay at eval\n- **Medical augmentations**: CLAHE, rotation, contrast, noise, elastic/grid distortion"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# TRAINING SETTINGS — adjust these as needed\n# ============================================================\n\nEPOCHS = 10           # 10 is enough with COCO-pretrained weights\nBATCH_SIZE = 64       # 64 for A100 80GB; use 32 for T4 16GB; 24 if OOM\nMAX_SAMPLES = None    # None = full dataset; set to 500 for quick test\nLEARNING_RATE = 1e-4  # Stable LR for Adam + detection models\nIMAGE_SIZE = 512      # Input image size\nVAL_FREQUENCY = 2     # Validate every N epochs (2 = 50% faster)\nEARLY_STOPPING = 5    # Stop after N validations without improvement\n\n# Advanced features (all enabled by default for max performance)\nFREEZE_EPOCHS = 3     # Freeze backbone for N epochs (0=disabled)\nSCHEDULER = \"cosine\"  # \"cosine\" or \"step\"\nGRAD_ACCUM = 1        # Gradient accumulation steps (increase if batch < 8)\n\n# ============================================================\n\nimport os, shutil\nfrom pathlib import Path\n\nn_cpus = os.cpu_count() or 4\nNUM_WORKERS = min(8, n_cpus)  # 8 workers per GPU for A100; min(4, cpus) for T4\n\n# Auto-detect previous run output saved as a dataset (for resume)\nPREV_RUN = None\nfor candidate in Path(\"/kaggle/input\").rglob(\"checkpoints\"):\n    if candidate.is_dir() and any(f.endswith(\".pth\") for f in os.listdir(candidate)):\n        PREV_RUN = str(candidate.parent)\n        break\n\nRESUME = False\nif PREV_RUN:\n    print(f\"Found previous run at: {PREV_RUN}\")\n\n    # Restore checkpoints and results\n    for folder in [\"checkpoints\", \"results\"]:\n        prev_folder = os.path.join(PREV_RUN, folder)\n        if os.path.exists(prev_folder):\n            dst = os.path.join(\"/kaggle/working\", folder)\n            os.makedirs(dst, exist_ok=True)\n            copied = 0\n            for f in os.listdir(prev_folder):\n                src_f = os.path.join(prev_folder, f)\n                dst_f = os.path.join(dst, f)\n                if not os.path.exists(dst_f) and os.path.isfile(src_f):\n                    shutil.copy2(src_f, dst_f)\n                    copied += 1\n            if copied:\n                print(f\"  Restored {copied} files to {folder}/\")\n    ckpt_dir = \"/kaggle/working/checkpoints\"\n    if os.path.exists(ckpt_dir) and os.listdir(ckpt_dir):\n        RESUME = True\n        print(f\"  Checkpoints: {sorted(os.listdir(ckpt_dir))}\")\n\n# Remove ALL old checkpoints — model architectures changed\nckpt_dir = \"/kaggle/working/checkpoints\"\nif os.path.exists(ckpt_dir):\n    for f in os.listdir(ckpt_dir):\n        if f.endswith(\".pth\"):\n            os.remove(os.path.join(ckpt_dir, f))\n            print(f\"  Removed incompatible checkpoint: {f}\")\n    RESUME = False  # Force fresh training\n\n# Figure out which models still need training\nMODELS_TO_TRAIN = []\nfor name in [\"fcos\", \"retinanet\", \"faster_rcnn\"]:\n    final = f\"/kaggle/working/checkpoints/{name}_final.pth\"\n    resume_ckpt = f\"/kaggle/working/checkpoints/{name}_resume.pth\"\n    if os.path.exists(final):\n        print(f\"  {name}: already completed\")\n    elif os.path.exists(resume_ckpt):\n        print(f\"  {name}: will RESUME from checkpoint\")\n        MODELS_TO_TRAIN.append(name)\n    else:\n        print(f\"  {name}: will train from scratch\")\n        MODELS_TO_TRAIN.append(name)\n\nresume_flag = \" --resume\" if RESUME else \"\"\n\nbase_args = (\n    f\" --data-dir {DATA_DIR}\"\n    f\" --epochs {EPOCHS} --batch-size {BATCH_SIZE}\"\n    f\" --lr {LEARNING_RATE} --image-size {IMAGE_SIZE}\"\n    f\" --val-frequency {VAL_FREQUENCY}\"\n    f\" --early-stopping {EARLY_STOPPING}\"\n    f\" --prefetch-factor 4\"\n    f\" --freeze-epochs {FREEZE_EPOCHS}\"\n    f\" --scheduler {SCHEDULER}\"\n    f\" --grad-accum {GRAD_ACCUM}\"\n    f\"{resume_flag}\"\n)\nif MAX_SAMPLES is not None:\n    base_args += f\" --max-samples {MAX_SAMPLES}\"\n\nprint(f\"\\nSettings: epochs={EPOCHS}, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\nprint(f\"Advanced: freeze={FREEZE_EPOCHS}ep, scheduler={SCHEDULER}, EMA=True, TTA=True, Soft-NMS=True\")\nprint(f\"Resume: {RESUME}, Models to train: {MODELS_TO_TRAIN or 'none (all done)'}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Train Models\n\nEach model now uses:\n- **COCO-pretrained backbone + FPN + heads** (only classification head replaced for 2 classes)\n- **Backbone freezing** (first 3 epochs): Protects pretrained low-level features\n- **EMA** (decay=0.999): Smoothed weights for better generalization\n- **Cosine annealing LR** with warmup: Better convergence profile\n- **WeightedRandomSampler**: 3x oversampling of positive patients\n- **Medical augmentations** (CLAHE, rotation, contrast, noise, elastic distortion)\n- **Gradient clipping** (max_norm=1.0): Prevents training instability\n- **Channels-last memory format**: 10-30% faster GPU convolutions\n\nAt evaluation time: **TTA** (horizontal flip) + **Gaussian Soft-NMS** for maximum AP.\n\n**If Kaggle kills the session**: Set `RESUME = True` above and re-run."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import subprocess, time, os\n\nt_start = time.time()\n\nif not MODELS_TO_TRAIN:\n    print(\"All models already trained! Skipping to evaluation.\")\nelif NUM_GPUS >= 2 and len(MODELS_TO_TRAIN) >= 2:\n    # GPU pool: as soon as a GPU finishes, start the next model immediately\n    # 256 CPUs / 8 GPUs = 32 CPUs available per GPU, use 8 workers each (I/O bound)\n    workers_per_gpu = min(8, max(4, (os.cpu_count() or 8) // NUM_GPUS))\n    pending = list(MODELS_TO_TRAIN)\n    running = {}   # gpu_id -> (model_name, process, log_file_handle, log_path)\n\n    print(f\"{'=' * 70}\")\n    print(f\"  GPU POOL: {NUM_GPUS} GPUs, {workers_per_gpu} workers/GPU\")\n    print(f\"  Models queued: {[m.upper() for m in pending]}\")\n    print(f\"{'=' * 70}\")\n\n    while pending or running:\n        # Launch on free GPUs\n        free_gpus = [g for g in range(NUM_GPUS) if g not in running]\n        while pending and free_gpus:\n            gpu_id = free_gpus.pop(0)\n            model_name = pending.pop(0)\n            log_path = f\"train_{model_name}.log\"\n            log_fh = open(log_path, \"w\")\n            cmd = (\n                f\"PYTHONUNBUFFERED=1 python main.py --mode train --model {model_name}\"\n                f\" --device cuda:{gpu_id} --num-workers {workers_per_gpu}\"\n                f\" {base_args}\"\n            )\n            proc = subprocess.Popen(cmd, shell=True, stdout=log_fh, stderr=subprocess.STDOUT)\n            running[gpu_id] = (model_name, proc, log_fh, log_path)\n            print(f\"  Started {model_name.upper()} on GPU {gpu_id}  (queue: {[m.upper() for m in pending]})\")\n\n        if not running:\n            break\n\n        # Wait and show progress\n        time.sleep(60)\n        print(f\"\\n--- Progress ({time.strftime('%H:%M:%S')}) ---\")\n\n        # Check for completed processes\n        for gpu_id in list(running.keys()):\n            model_name, proc, log_fh, log_path = running[gpu_id]\n            rc = proc.poll()\n\n            # Show latest log line\n            try:\n                with open(log_path) as f:\n                    lines = f.readlines()\n                for line in reversed(lines):\n                    stripped = line.strip()\n                    if stripped and not stripped.startswith(\"W0\"):\n                        print(f\"  [{model_name}] GPU {gpu_id}: {stripped}\")\n                        break\n            except FileNotFoundError:\n                print(f\"  [{model_name}] GPU {gpu_id}: (waiting...)\")\n\n            if rc is not None:\n                # Process finished — free the GPU\n                log_fh.close()\n                del running[gpu_id]\n                status = \"DONE\" if rc == 0 else f\"FAILED (exit {rc})\"\n                print(f\"\\n{'=' * 70}\")\n                print(f\"  {model_name.upper()} — {status} (GPU {gpu_id} now free)\")\n                print(f\"{'=' * 70}\")\n                try:\n                    with open(log_path) as f:\n                        for line in f.readlines()[-15:]:\n                            print(line, end=\"\")\n                except FileNotFoundError:\n                    pass\n\n    # Print final summary\n    for model_name in MODELS_TO_TRAIN:\n        log_path = f\"train_{model_name}.log\"\n        if os.path.exists(log_path):\n            with open(log_path) as f:\n                lines = f.readlines()\n            # Find the last epoch summary line\n            for line in reversed(lines):\n                if f\"[{model_name}]\" in line and \"Epoch\" in line:\n                    print(f\"  {line.strip()}\")\n                    break\nelse:\n    # Sequential on single GPU\n    for model_name in MODELS_TO_TRAIN:\n        print(f\"\\n{'=' * 70}\")\n        print(f\"  Training: {model_name.upper()}\")\n        print(f\"{'=' * 70}\")\n        !PYTHONUNBUFFERED=1 python main.py --mode train --model {model_name} \\\n            --device cuda:0 --num-workers {NUM_WORKERS} {base_args}\n\nelapsed = time.time() - t_start\nhours = int(elapsed // 3600)\nmins = int((elapsed % 3600) // 60)\nprint(f\"\\n{'=' * 70}\")\nprint(f\"  TRAINING COMPLETE  —  Total time: {hours}h {mins}m\")\nprint(f\"{'=' * 70}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Evaluate & Generate Plots"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate all models and generate plots/visualizations\n!python main.py --mode evaluate --model all --num-workers {NUM_WORKERS} {base_args}\n!python main.py --mode compare --num-workers {NUM_WORKERS} {base_args}\n!python main.py --mode visualize --num-workers {NUM_WORKERS} {base_args}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load and display metrics\nimport json\n\nwith open(\"results/all_metrics.json\") as f:\n    metrics = json.load(f)\n\nprint(\"=\" * 70)\nprint(\"  DETECTION PERFORMANCE (%)\")\nprint(\"=\" * 70)\nprint(f\"{'Model':<16} {'AP@0.5':>8} {'AP@.5:.95':>10} {'AP_M':>8} {'AP_L':>8} {'AR@10':>8} {'AR_L':>8}\")\nprint(\"-\" * 70)\nfor name, m in metrics.items():\n    print(f\"{name:<16} {m['AP@0.5']*100:>8.1f} {m['AP@0.5:0.95']*100:>10.1f}\"\n          f\" {m['AP_M']*100:>8.1f} {m['AP_L']*100:>8.1f}\"\n          f\" {m['AR@10']*100:>8.1f} {m['AR_L']*100:>8.1f}\")\n\nprint()\nprint(\"=\" * 70)\nprint(\"  PATIENT-LEVEL CLASSIFICATION (%)\")\nprint(\"=\" * 70)\nprint(f\"{'Model':<16} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10}\")\nprint(\"-\" * 70)\nfor name, m in metrics.items():\n    print(f\"{name:<16} {m['patient_accuracy']*100:>10.1f} {m['patient_precision']*100:>10.1f}\"\n          f\" {m['patient_recall']*100:>10.1f} {m['patient_f1']*100:>10.1f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation AP@0.5 Over Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from IPython.display import Image, display\ndisplay(Image(filename=\"results/val_ap_over_epochs.png\", width=800))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP & AR Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(Image(filename=\"results/ap_comparison.png\", width=800))\n",
    "display(Image(filename=\"results/ar_comparison.png\", width=800))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(Image(filename=\"results/pr_curve.png\", width=600))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP vs IoU Threshold"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import os\nfrom IPython.display import Image, display\nif os.path.exists(\"results/ap_vs_iou.png\"):\n    display(Image(filename=\"results/ap_vs_iou.png\", width=800))\nelse:\n    print(\"ap_vs_iou.png not generated (requires predictions — run 'compare' mode).\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient-Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(Image(filename=\"results/classification_metrics.png\", width=800))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Speed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(Image(filename=\"results/epoch_times.png\", width=600))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Samples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(Image(filename=\"results/detection_samples.png\", width=900))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Download Results\n\nOn Kaggle, results are automatically saved as notebook output. You can also download the ZIP."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Package results for download\n",
    "!zip -r /kaggle/working/pneumonia_results.zip results/ checkpoints/\n",
    "print(\"Results packaged. Download from the 'Output' tab on the right.\")\n",
    "print(\"Extract into your local Project_Pneumonia_Detection/ folder.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}