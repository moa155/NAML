{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d46bf83",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "We will explore how the **PageRank algorithm** works and how it relates to real-world web traffic data.\n",
    "\n",
    "The dataset you will use is derived from the **Wikipedia category 'Machine Learning'**. Each node corresponds to a Wikipedia article, and each directed edge represents a hyperlink between two pages. Additionally, a separate file contains the **page view statistics (traffic)** for each article. The data was downloaded using Wikipedia API, for the curious the scripts are in the `datagen` folder.\n",
    "\n",
    "Your goals are:\n",
    "1. Load and inspect the dataset (`nodes.csv`, `edges.csv`, and `traffic.csv`).\n",
    "2. Build a directed graph using NetworkX.\n",
    "3. Compute the PageRank vector using both the NetworkX implementation and the power iteration method.\n",
    "4. Compare the results of both methods and visualize the graph.\n",
    "5. Compare PageRank values with real page traffic and discuss their correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c888322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "nodes = pd.read_csv(\"nodes.csv\")\n",
    "edges = pd.read_csv(\"edges.csv\")\n",
    "traffic = pd.read_csv(\"traffic.csv\")\n",
    "\n",
    "print(f\"Loaded {len(nodes)} nodes, {len(edges)} edges, and {len(traffic)} traffic entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60300b4",
   "metadata": {},
   "source": [
    "### Build the Graph and Transition Matrix\n",
    "\n",
    "We first create a **directed graph** representing the hyperlink structure between Wikipedia articles.\n",
    "Each node corresponds to a page, and each directed edge represents a link from one page to another.\n",
    "\n",
    "Then we construct the **transition matrix** $M$ such that $M_{ij}$ represents the probability of moving from page $j$ to page $i$ when following a link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "graph.add_nodes_from(nodes[\"node\"])\n",
    "graph.add_edges_from(edges[[\"source\", \"target\"]].values)\n",
    "\n",
    "N = len(nodes)\n",
    "node_index = nodes[\"node\"].to_dict()\n",
    "node_index = {node_index[k]: k for k in node_index}\n",
    "\n",
    "# Build the stochastic matrix M\n",
    "M = np.zeros((N, N))\n",
    "for u, v in graph.edges():\n",
    "    if graph.out_degree(u) > 0:\n",
    "        M[node_index[v], node_index[u]] = 1.0 / graph.out_degree(u)\n",
    "\n",
    "print(f\"Transition matrix M built with shape: {M.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc656be",
   "metadata": {},
   "source": [
    "### The Damping Factor and Power Iteration\n",
    "\n",
    "The **damping factor** (usually denoted as $d$) models the probability that a user follows a link versus jumping to a random page.\n",
    "Typically, $d = 0.85$ means the user follows a link 85% of the time and jumps randomly 15% of the time.\n",
    "\n",
    "We will:\n",
    "- Compute PageRank using NetworkX's built-in implementation.\n",
    "- Compute PageRank manually using the **power iteration method** applied to the Google matrix \n",
    "$$G = d M + \\frac{1 - d}{N} 1_{N\\times N}$$\n",
    "- Compare the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.85\n",
    "\n",
    "# --- Compute PageRank using NetworkX ---\n",
    "pagerank = nx.pagerank(graph, alpha=damping_factor)\n",
    "pagerank_vec = np.array([pagerank[n] for n in nodes[\"node\"]])\n",
    "nx.set_node_attributes(graph, pagerank, \"pagerank\")\n",
    "\n",
    "# --- Compute PageRank using power iteration ---\n",
    "# SOLUTION-BEGIN\n",
    "G_matrix = damping_factor * M + (1 - damping_factor) / N * np.ones((N, N))\n",
    "\n",
    "p = np.ones(N) / N\n",
    "tol = 1e-10\n",
    "max_iter = 1000\n",
    "\n",
    "for i in range(max_iter):\n",
    "    p_next = G_matrix @ p\n",
    "    p_next /= np.linalg.norm(p_next)\n",
    "    if np.linalg.norm(p_next - p, 2) < tol:\n",
    "        print(f\"Converged after {i} iterations\")\n",
    "        break\n",
    "    p = p_next\n",
    "\n",
    "pagerank_powit = p / np.sum(p)\n",
    "# SOLUTION-END\n",
    "\n",
    "# Compare results\n",
    "corr = np.corrcoef(pagerank_vec, pagerank_powit)[0, 1]\n",
    "l1_diff = np.linalg.norm(pagerank_vec - pagerank_powit, 1)\n",
    "\n",
    "print(\"\\n--- PageRank Comparison ---\")\n",
    "print(f\"Pearson correlation: {corr:.6f}\")\n",
    "print(f\"L1 difference:       {l1_diff:.6e}\")\n",
    "\n",
    "plt.plot(pagerank_vec, 'o-')\n",
    "plt.plot(pagerank_powit, 'x-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b20ff1",
   "metadata": {},
   "source": [
    "### Visualize the Graph\n",
    "\n",
    "We now visualize the graph using a spring layout. Node sizes are proportional to their PageRank values.\n",
    "\n",
    "- Larger nodes correspond to more 'important' pages.\n",
    "- The layout helps highlight clusters and central pages within the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "pos = nx.spring_layout(graph, k=0.5, seed=42)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(\n",
    "    graph,\n",
    "    pos,\n",
    "    node_size=pagerank_vec * 5000,\n",
    "    node_color=\"skyblue\",\n",
    "    alpha=0.8,\n",
    "    edgecolors=\"gray\",\n",
    ")\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(graph, pos, arrows=True, alpha=0.4)\n",
    "\n",
    "# Draw labels (of most important pages)optional for smaller graphs\n",
    "nx.draw_networkx_labels(\n",
    "    graph,\n",
    "    pos,\n",
    "    labels={\n",
    "        n: n for n in nodes[\"node\"] if pagerank[n] > np.quantile(pagerank_vec, 0.9)\n",
    "    },\n",
    "    font_color=\"k\",\n",
    "    font_size=10,\n",
    "    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3')\n",
    ")\n",
    "\n",
    "plt.title(\"Wikipedia Machine Learning Graph\\nNode size = PageRank\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf9912",
   "metadata": {},
   "source": [
    "### Compare PageRank with Wikipedia Traffic\n",
    "\n",
    "Finally, we compare the computed PageRank values with **real page traffic** obtained from the Wikimedia API.\n",
    "This allows us to see whether the theoretical importance (PageRank) corresponds to actual user visits.\n",
    "\n",
    "- We add the PageRank value to the traffic DataFrame.\n",
    "- Plot traffic vs. PageRank on a log-log scale.\n",
    "- Compute and print their correlation.\n",
    "- Print the 10 most important pages (highest pagerank score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic[\"pagerank\"] = traffic[\"node\"].map(pagerank)\n",
    "\n",
    "# SOLUTION-BEGIN\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(traffic.traffic, traffic.pagerank, alpha=0.7)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Wikipedia Traffic (log scale)\")\n",
    "plt.ylabel(\"PageRank (log scale)\")\n",
    "plt.title(\"Correlation between PageRank and Traffic\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr_traffic = np.corrcoef(traffic.traffic, traffic.pagerank)[0, 1]\n",
    "print(f\"Correlation between traffic and pagerank: {corr_traffic:.3f}\")\n",
    "\n",
    "print(\"\\nTop 10 pages by PageRank:\")\n",
    "print(traffic.nlargest(10, \"pagerank\")[[\"node\", \"pagerank\", \"traffic\"]])\n",
    "# SOLUTION-END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
