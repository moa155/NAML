{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Regression (SVR) Hands-on Exercise\n",
        "\n",
        "In this exercise, we will implement and train a Support Vector Regression model using JAX and evaluate its performance on synthetic data. We will go through the steps of data generation, model training, and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Import Libraries\n",
        "\n",
        "We will begin by importing the necessary libraries. JAX will be used for numerical computations, NumPy for array manipulations, and Matplotlib for visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define the SVR Class\n",
        "\n",
        "We will create a class `SVR` to encapsulate the SVR model. The class will include methods for calculating the loss, training the model, and making predictions.\n",
        "\n",
        "#### Loss Function\n",
        "\n",
        "The loss function for SVR is defined as (soft-margin):\n",
        "\n",
        "$$ L(w) = \\lambda \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, |f(x_i) - y_i| - \\epsilon) $$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ w $ are the model parameters.\n",
        "- $ \\lambda $ is the regularization parameter.\n",
        "- $ \\epsilon $ is the epsilon-insensitive loss threshold.\n",
        "- $ f(x_i) $ is the predicted value for input $ x_i $.\n",
        "- $ y_i $ is the actual target value.\n",
        "\n",
        "Let's define the `SVR` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SVR:\n",
        "    def __init__(self, epsilon=0.1, lmbda=1.0):\n",
        "        self.epsilon = epsilon\n",
        "        self.lmbda = lmbda\n",
        "        self.w = None\n",
        "\n",
        "    def loss(self, params, X, y):\n",
        "        # SOLUTION-BEGIN\n",
        "        predictions = X.reshape((-1, self.n_features)) @ params[:-1] + params[-1]\n",
        "\n",
        "        # Compute epsilon-insensitive loss\n",
        "        epsilon_loss = jnp.maximum(0, jnp.abs(predictions - y) - self.epsilon)\n",
        "\n",
        "        # Regularization term (L2 norm of w)\n",
        "        reg_term = self.lmbda * jnp.sum(params**2)\n",
        "\n",
        "        # Total loss\n",
        "        return reg_term + jnp.mean(epsilon_loss)\n",
        "        # SOLUTION-END\n",
        "\n",
        "    def train(self, X, y, lr=1e-2, max_iter=1000):\n",
        "        # SOLUTION-BEGIN\n",
        "        _, self.n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.w = jnp.zeros(self.n_features + 1)\n",
        "\n",
        "        # Solve optimization problem\n",
        "        grad_fn = jax.grad(self.loss, argnums=0)\n",
        "\n",
        "        @jax.jit\n",
        "        def step(w):\n",
        "            return w - lr * grad_fn(w, X, y)\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            self.w = step(self.w)\n",
        "        # SOLUTION-END\n",
        "\n",
        "    def predict(self, X):\n",
        "        # SOLUTION-BEGIN\n",
        "        return X.reshape((-1, self.n_features)) @ self.w[:-1] + self.w[-1]\n",
        "        # SOLUTION-END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Generate Synthetic Data\n",
        "\n",
        "Next, we will generate synthetic data for regression. We will create a linear relationship with added Gaussian noise.\n",
        "\n",
        "- We will generate $ n\\_{samples} $ data points.\n",
        "- The true relationship will follow the equation $ y = mx + c $.\n",
        "- Gaussian noise will be added to simulate real-world data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)  # For reproducibility\n",
        "m = 2.5  # Slope\n",
        "c = 1.0  # Intercept\n",
        "n_samples = 100\n",
        "\n",
        "# Generate X values uniformly distributed between 0 and 10\n",
        "X = np.random.uniform(0, 10, size=(n_samples, 1))\n",
        "\n",
        "# Create the line (y = mx + c) and add Gaussian noise\n",
        "noise = np.random.normal(0, 1, size=(n_samples, 1))  # Gaussian noise\n",
        "y = m * X + c + noise  # Add noise to the true line\n",
        "y = y.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Split the Data\n",
        "\n",
        "Now, we will split the dataset into training and testing sets. This allows us to evaluate the model's performance on unseen data.\n",
        "\n",
        "We will use an 80-20 split for training and testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train = jnp.array(X_train)\n",
        "X_test = jnp.array(X_test)\n",
        "y_train = jnp.array(y_train)\n",
        "y_test = jnp.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Train the SVR Model\n",
        "\n",
        "We will now create an instance of the `SVR` class and train the model using the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svr = SVR(epsilon=1.0, lmbda=0.1)\n",
        "svr.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Make Predictions\n",
        "\n",
        "After training, we will use the model to make predictions on both the training and testing datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_train = svr.predict(X_train)\n",
        "y_pred_test = svr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Evaluate the Model\n",
        "\n",
        "We will evaluate the model's performance using the Mean Squared Error (MSE), which is defined as:\n",
        "\n",
        "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ y_i $ is the actual value.\n",
        "- $ \\hat{y}_i $ is the predicted value.\n",
        "- $ n $ is the number of samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse_train = jnp.mean((y_train - y_pred_train) ** 2)\n",
        "mse_test = jnp.mean((y_test - y_pred_test) ** 2)\n",
        "print(f\"Train MSE: {mse_train:.4f}\")\n",
        "print(f\"Test MSE: {mse_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Visualize the Results\n",
        "\n",
        "Finally, we will visualize the training data, test data, and the model's predictions. This will help us understand how well our model has captured the underlying relationship.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training data\n",
        "plt.scatter(X_train, y_train, color=\"blue\", label=\"Training data\")\n",
        "# Plot test data\n",
        "plt.scatter(X_test, y_test, color=\"green\", marker=\"x\", label=\"Test data\")\n",
        "\n",
        "# Plot the prediction line (SVR model)\n",
        "x_range = jnp.linspace(0, 10, 100)\n",
        "y_pred_line = svr.predict(x_range)\n",
        "plt.plot(x_range, y_pred_line, color=\"red\", label=\"SVR prediction\")\n",
        "plt.fill_between(\n",
        "    x_range,\n",
        "    y_pred_line - svr.epsilon,\n",
        "    y_pred_line + svr.epsilon,\n",
        "    label=\"SVR tube\",\n",
        "    color=\"r\",\n",
        "    alpha=0.1,\n",
        ")\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"SVR on Synthetic Data with Gaussian Noise\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "We will change the previous code to implement a linear Support Vector Machine in primal formulation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define the SVM Class\n",
        "\n",
        "We will create a class `SVM` to encapsulate the SVM model. The class will include methods for calculating the loss, training the model, and making predictions.\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "The loss function for SVM is defined as the hinge loss:\n",
        "\n",
        "$$ L(w) = \\lambda \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i (w^T x_i + b)) $$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ w $ are the model parameters (weights).\n",
        "- $ b $ is the bias term.\n",
        "- $ \\lambda $ is the regularization parameter.\n",
        "- $ y_i $ is the true label for the sample $ i $.\n",
        "- $ x_i $ is the feature vector for the sample $ i $.\n",
        "- The first term is the regularization term, and the second term is the hinge loss.\n",
        "\n",
        "Let's define the `SVM` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SVM:\n",
        "    def __init__(self, lmbda=1.0):\n",
        "        self.lmbda = lmbda\n",
        "        self.w = None\n",
        "\n",
        "    def loss(self, params, X, y):\n",
        "        # SOLUTION-BEGIN\n",
        "        # Compute the decision function\n",
        "        print(X.shape, params.shape, y.shape)\n",
        "        decision = X @ params[:-1] + params[-1]\n",
        "        # Compute the hinge loss\n",
        "        loss_val = jnp.maximum(0, 1 - y * decision)\n",
        "        # Regularization term (L2 norm of w)\n",
        "        reg_term = self.lmbda * jnp.sum(params**2)\n",
        "        # Total loss\n",
        "        return reg_term + jnp.mean(loss_val)\n",
        "        # SOLUTION-END\n",
        "\n",
        "    def train(self, X, y, lr=1e-2, max_iter=1000):\n",
        "        # SOLUTION-BEGIN\n",
        "        _, self.n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.w = jnp.zeros(self.n_features + 1)\n",
        "\n",
        "        # Solve optimization problem\n",
        "        grad_fn = jax.grad(self.loss, argnums=0)\n",
        "\n",
        "        @jax.jit\n",
        "        def step(w):\n",
        "            return w - lr * grad_fn(w, X, y)\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            self.w = step(self.w)\n",
        "        # SOLUTION-END\n",
        "\n",
        "    def predict(self, X):\n",
        "        # SOLUTION-BEGIN\n",
        "        # Decision function\n",
        "        decision = X @ self.w[:-1] + self.w[-1]\n",
        "        return jnp.sign(decision)\n",
        "        # SOLUTION-END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Generate Synthetic Classification Data\n",
        "\n",
        "Next, we will generate synthetic data for classification. We will create a dataset of points in a 2D space and label them based on their coordinates.\n",
        "\n",
        "- We will generate $n_{samples} $ data points.\n",
        "- The labels will be determined by the condition $ x_1 + x_2 > 10 $ to classify points into two categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "\n",
        "# Generate X values uniformly distributed between [0, 10]^2\n",
        "X = np.random.uniform(0, 10, size=(n_samples, 2))\n",
        "\n",
        "# Generate binary labels\n",
        "y = np.where(X.sum(axis=1) > 10, 1, -1)\n",
        "y = y.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Split the Data\n",
        "\n",
        "Now, we will split the dataset into training and testing sets. This allows us to evaluate the model's performance on unseen data.\n",
        "\n",
        "We will use an 80-20 split for training and testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train = jnp.array(X_train)\n",
        "X_test = jnp.array(X_test)\n",
        "y_train = jnp.array(y_train)\n",
        "y_test = jnp.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Train the SVM Model\n",
        "\n",
        "We will now create an instance of the `SVM` class and train the model using the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm = SVM(lmbda=0.001)\n",
        "svm.train(X_train, y_train, lr=1e-1, max_iter=5000)\n",
        "print(\"Loss: \", svm.loss(svm.w, X_train, y_train))\n",
        "print(svm.w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Make Predictions\n",
        "\n",
        "After training, we will use the model to make predictions on both the training and testing datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_train = svm.predict(X_train)\n",
        "y_pred_test = svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Evaluate the Model\n",
        "\n",
        "We will evaluate the model's performance using accuracy, which is defined as the proportion of correct predictions:\n",
        "\n",
        "$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$\n",
        "\n",
        "Let's calculate and print the accuracy for both the training and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_train = jnp.mean(y_pred_train == y_train)\n",
        "accuracy_test = jnp.mean(y_pred_test == y_test)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Visualize the Results\n",
        "\n",
        "Finally, we will visualize the training data, test data, and the decision boundary of the SVM model. This will help us understand how well our model has separated the classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label=\"Training data\", marker=\"o\")\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=\"Test data\", marker=\"x\")\n",
        "\n",
        "# Plot the decision boundary\n",
        "t = jnp.linspace(0, 10, 1000)\n",
        "xx1, xx2 = jnp.meshgrid(t, t)\n",
        "xx = jnp.stack([xx1.flatten(), xx2.flatten()], axis=1)\n",
        "yy = svm.predict(xx)\n",
        "plt.contourf(xx1, xx2, yy.reshape(xx1.shape), alpha=0.1)\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.title(\"SVM Decision Boundary on Synthetic Data\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
