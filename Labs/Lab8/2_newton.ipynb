{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZCDNUgw0nI3"
   },
   "source": [
    "# Newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1670934505648,
     "user_tz": -60
    },
    "id": "IJcJCwFhc8cs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "# We enable double precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qkdJdGg1AP1"
   },
   "source": [
    "We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n",
    "We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670934505649,
     "user_tz": -60
    },
    "id": "c0h8ihCddDPf"
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "np.random.seed(0)\n",
    "A = np.random.randn(n, n)\n",
    "x_ex = np.random.randn(n)\n",
    "b = A @ x_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UanVhF4xAVoX"
   },
   "source": [
    "Define the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1670934506647,
     "user_tz": -60
    },
    "id": "5bIQ9Y3CdbwW",
    "outputId": "3d16e09e-dacf-4d65-8f07-28d00b69c847"
   },
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.square(A @ x - b))\n",
    "\n",
    "\n",
    "print(loss(x_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAZ9XGaiAs3X"
   },
   "source": [
    "By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670934506647,
     "user_tz": -60
    },
    "id": "KflmuLXld2T4"
   },
   "outputs": [],
   "source": [
    "grad = jax.grad(loss)\n",
    "hess = jax.jacfwd(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSMg8ocDBndO"
   },
   "source": [
    "Check that the results are correct (up to machine precision).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1264,
     "status": "ok",
     "timestamp": 1670934510887,
     "user_tz": -60
    },
    "id": "xZulGRQ1efFP",
    "outputId": "2876af07-7320-4946-c2d2-31b75d849733"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x_guess = np.random.randn(n)\n",
    "\n",
    "G_ad = grad_jit(x_guess)\n",
    "G_ex = 2 * A.T @ (A @ x_guess - b)\n",
    "print(np.linalg.norm(G_ad - G_ex))\n",
    "\n",
    "H_ad = hess_jit(x_guess)\n",
    "H_ex = 2 * A.T @ A\n",
    "print(np.linalg.norm(H_ad - H_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-gA_kKPB2SV"
   },
   "source": [
    "\n",
    "* JAX enables us to compute the Hessian $\\nabla^2 \\mathcal{J}(\\mathbf{x})$. This is a **function**.\n",
    "* Then, we use it by evaluating the Hessian at one point $\\nabla^2 \\mathcal{J}(\\mathbf{x})|_{\\mathbf{x}_0}$, for instance when we apply the Newton's method. This is a **matrix**.\n",
    "* However, when we use this matrix (the evaluation of the Hessian at a point), it rarely happens that we need the whole structure of the matrix. In most case it is sufficient to know the action that the matrix has on a vector $\\mathbf{v}_0$. **Avoiding storing a full matrix greatly diminishes the cost in memory (which is quadratic).**\n",
    "\n",
    "In other words, do we need to compute $\\nabla^2 \\mathcal{J}(\\mathbf{x})|_{\\mathbf{x}_0}$ in order to compute $\\nabla^2 \\mathcal{J}(\\mathbf{x})|_{\\mathbf{x}_0} \\mathbf{v}_0$? \n",
    "\n",
    "From calcolus we know that\n",
    "\n",
    "$$\n",
    "\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n",
    "$$\n",
    "\n",
    "because\n",
    "\n",
    "$$\n",
    "\\left( \\nabla_{\\mathbf x}\\phi(\\mathbf x,\\mathbf v) \\right)_i\n",
    "= \\frac{\\partial}{\\partial x_i}\n",
    "\\left( \\sum_{j=1}^n \\frac{\\partial \\mathcal J}{\\partial x_j}(\\mathbf x)\\, v_j \\right)\n",
    "= \\sum_{j=1}^n \\frac{\\partial^2 \\mathcal J}{\\partial x_i \\partial x_j}(\\mathbf x)\\, v_j .\n",
    "$$\n",
    "\n",
    "Then, in JAX we can compute $\\nabla^2 \\mathcal{J}(\\mathbf{x})|_{\\mathbf{x}_0} \\mathbf{v}_0$ by evaluating at $(\\mathbf{x}_0, \\mathbf{v}_0)$ the function\n",
    "$$\n",
    "(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla_{\\mathbf{x}} (\\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}).\n",
    "$$\n",
    "\n",
    "Notice that in this implementation there is never a matrix since $\\mathcal{J}$ is a scalar field.\n",
    "\n",
    "Implement this version of the Hessian and compare the computational performance w.r.t. the full hessian computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1670934517725,
     "user_tz": -60
    },
    "id": "T9969dU4kc6f",
    "outputId": "1f6d6a58-d1ea-4b0c-cfe4-6609d995d0d2"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "v = np.random.randn(n)\n",
    "\n",
    "hvp_basic = lambda x, v: hess(x) @ v\n",
    "\n",
    "# SOLUTION-BEGIN\n",
    "gvp = lambda x, v: jnp.dot(grad(x), v)\n",
    "hvp = lambda x, v: jax.grad(gvp, argnums=0)(x, v)\n",
    "\n",
    "hvp_basic_jit = jax.jit(hvp_basic)\n",
    "hvp_jit = jax.jit(hvp)\n",
    "# SOLUTION-END\n",
    "\n",
    "Hv_ad = hvp_jit(x_guess, v)\n",
    "Hv_ex = H_ex @ v\n",
    "print(np.linalg.norm(Hv_ad - Hv_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17317,
     "status": "ok",
     "timestamp": 1670743186289,
     "user_tz": -60
    },
    "id": "jsA4eUnuj3ju",
    "outputId": "cbd398c7-dfc3-4ced-ade7-4cbbb7a280a7"
   },
   "outputs": [],
   "source": [
    "%timeit hvp_basic_jit(x_guess, v)\n",
    "%timeit hvp_jit(x_guess, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TagmrdjG4Ww4"
   },
   "source": [
    "Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670743186290,
     "user_tz": -60
    },
    "id": "jZQBCmXBhBwu",
    "outputId": "76d4bb2a-f683-422c-adee-978deaba1511"
   },
   "outputs": [],
   "source": [
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # SOLUTION-BEGIN\n",
    "    H = hess_jit(x)\n",
    "    G = grad_jit(x)\n",
    "    incr = np.linalg.solve(H, -G)\n",
    "    x += incr\n",
    "    # SOLUTION-END\n",
    "\n",
    "    print(\"============ epoch %d\" % epoch)\n",
    "    print(\"loss: %1.3e\" % loss_jit(x))\n",
    "    print(\"grad: %1.3e\" % np.linalg.norm(G))\n",
    "    print(\"incr: %1.3e\" % np.linalg.norm(incr))\n",
    "\n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        break\n",
    "\n",
    "rel_err = np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex)\n",
    "print(f\"Relative error: {rel_err:1.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the system with `jax.scipy.sparse.linalg.cg` and use the \"matrix-free\" version of the Hessia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # SOLUTION-BEGIN\n",
    "    G = grad_jit(x)\n",
    "    incr, info = jax.scipy.sparse.linalg.cg(lambda y: hvp_jit(x, y), -G, tol=eps)\n",
    "    x += incr\n",
    "    # SOLUTION-END\n",
    "\n",
    "    print(\"============ epoch %d\" % epoch)\n",
    "    print(\"loss: %1.3e\" % loss_jit(x))\n",
    "    print(\"grad: %1.3e\" % np.linalg.norm(G))\n",
    "    print(\"incr: %1.3e\" % np.linalg.norm(incr))\n",
    "\n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        break\n",
    "\n",
    "rel_err = np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex)\n",
    "print(f\"Relative error: {rel_err:1.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNL7303C4oTL"
   },
   "source": [
    "Repeat the optimization loop for the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^4\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOZZWrn3iEgI"
   },
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum((A @ x - b) ** 4)\n",
    "\n",
    "\n",
    "grad = jax.grad(loss)\n",
    "hess = jax.jacfwd(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1670743187191,
     "user_tz": -60
    },
    "id": "-hZyFgjQiOvB",
    "outputId": "3d280332-6603-450f-a977-0fc768fd8be8"
   },
   "outputs": [],
   "source": [
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "hist = [loss_jit(x)]\n",
    "for epoch in range(num_epochs):\n",
    "    hist.append(loss_jit(x))\n",
    "\n",
    "    H = hess_jit(x)\n",
    "    G = grad_jit(x)\n",
    "    incr = np.linalg.solve(H, -G)\n",
    "    x += incr\n",
    "    \n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        print(\"convergence reached!\")\n",
    "        break\n",
    "\n",
    "plt.semilogy(hist, \"o-\")\n",
    "print(\"epochs: %d\" % epoch)\n",
    "print(\"relative error: %1.3e\" % (np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-Newton Optimization with BFGS Update\n",
    "\n",
    "This algorithm minimizes an objective function $ f(\\mathbf{x}) $ using a quasi-Newton method with the BFGS update. The goal is to iteratively update the solution $ \\mathbf{x} $ and approximate the inverse Hessian until convergence criteria are met.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. **Initialization**:\n",
    "\n",
    "   - Set the initial guess $ \\mathbf{x} = \\mathbf{x}\\_{\\text{guess}} $.\n",
    "   - Let $ \\mathbf{I} $ be the identity matrix, and initialize $ B^{-1} = \\mathbf{I} $.\n",
    "   - Compute the initial gradient $ \\nabla f = \\nabla f(\\mathbf{x}\\_{\\text{guess}}) $.\n",
    "   - Initialize the loss history: $ \\text{history} = [f(\\mathbf{x}_{\\text{guess}})] $.\n",
    "   - Set $ \\text{epoch} = 0 $.\n",
    "\n",
    "   $$\n",
    "   B^{-1} = \\mathbf{I}, \\quad \\nabla f = \\nabla f(\\mathbf{x}_{\\text{guess}}), \\quad \\text{history} = [f(\\mathbf{x}_{\\text{guess}})]\n",
    "   $$\n",
    "\n",
    "2. **Iterative Updates**:\n",
    "\n",
    "   - While $ \\|\\nabla f\\| > \\text{tol} $ and epoch $<$ max epoch:\n",
    "\n",
    "     - Increment the epoch counter:\n",
    "\n",
    "       $$\n",
    "       \\text{epoch} \\leftarrow \\text{epoch} + 1\n",
    "       $$\n",
    "\n",
    "     - Compute the search direction:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{p} = -B^{-1} \\nabla f\n",
    "       $$\n",
    "\n",
    "     - Perform a line search to find the step size $ \\alpha $ using `sp.optimize.line_search`:\n",
    "\n",
    "       $$\n",
    "       \\alpha \\leftarrow \\text{line\\_search}(f, \\nabla f, \\mathbf{x}, \\mathbf{p})\n",
    "       $$\n",
    "\n",
    "       If $ \\alpha $ is not found (scipy returns `None`), set $ \\alpha = 10^{-8} $.\n",
    "\n",
    "     - Update the solution vector:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\alpha \\mathbf{p}\n",
    "       $$\n",
    "\n",
    "     - Compute the displacement:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{s} = \\mathbf{x}_{\\text{new}} - \\mathbf{x}\n",
    "       $$\n",
    "\n",
    "     - Update $ \\mathbf{x} $:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{x} \\leftarrow \\mathbf{x}_{\\text{new}}\n",
    "       $$\n",
    "\n",
    "     - Compute the new gradient and gradient difference:\n",
    "\n",
    "       $$\n",
    "       \\nabla f_{\\text{new}} = \\nabla f(\\mathbf{x}), \\quad \\mathbf{y} = \\nabla f_{\\text{new}} - \\nabla f\n",
    "       $$\n",
    "\n",
    "       Update $ \\nabla f $:\n",
    "\n",
    "       $$\n",
    "       \\nabla f \\leftarrow \\nabla f_{\\text{new}}\n",
    "       $$\n",
    "\n",
    "     - Compute the scalar $ \\rho $:\n",
    "\n",
    "       $$\n",
    "       \\rho = \\frac{1}{\\mathbf{y}^\\top \\mathbf{s}}\n",
    "       $$\n",
    "\n",
    "     - Update the inverse Hessian approximation $ B^{-1} $ using the Sherman–Morrison formula:\n",
    "\n",
    "       $$\n",
    "       \\mathbf{E} = \\mathbf{I} - \\rho \\mathbf{y} \\mathbf{s}^\\top\n",
    "       $$\n",
    "\n",
    "       $$\n",
    "       B^{-1} \\leftarrow \\mathbf{E}^\\top B^{-1} \\mathbf{E} + \\rho \\mathbf{s} \\mathbf{s}^\\top\n",
    "       $$\n",
    "\n",
    "     - Append the current loss value to history:\n",
    "       $$\n",
    "       \\text{history.append}(f(\\mathbf{x}))\n",
    "       $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000\n",
    "tol = 1e-8\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "epoch = 0\n",
    "x = x_guess.copy()\n",
    "I = np.eye(x.size)\n",
    "Binv = I\n",
    "grad = grad_jit(x_guess)\n",
    "history = [loss_jit(x_guess)]\n",
    "\n",
    "while np.linalg.norm(grad) > tol and epoch < max_epochs:\n",
    "    epoch += 1\n",
    "\n",
    "    # SOLUTION-BEGIN\n",
    "    # search direction\n",
    "    p = -Binv @ grad\n",
    "\n",
    "    # line search\n",
    "    alpha = sp.optimize.line_search(loss_jit, grad_jit, x, p)[0]\n",
    "    alpha = 1e-8 if alpha is None else alpha\n",
    "    x_new = x + alpha * p\n",
    "\n",
    "    # computing y and s\n",
    "    s = x_new - x\n",
    "    x = x_new\n",
    "    grad_new = grad_jit(x_new)\n",
    "    y = grad_new - grad\n",
    "    grad = grad_new\n",
    "\n",
    "    # Sherman–Morrison update\n",
    "    rho = 1.0 / (np.dot(y, s))\n",
    "    E = I - rho * np.outer(y, s)\n",
    "    Binv = E.T @ Binv @ E + rho * np.outer(s, s)\n",
    "    # SOLUTION-END\n",
    "\n",
    "    history.append(loss_jit(x))\n",
    "\n",
    "plt.semilogy(history, \"o-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOsWkGVhufqPYWuaAVLQS75",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
